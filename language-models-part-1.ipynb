{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text2 = sense and sensibility by jane austen\n",
    "unigram_counts = Counter(text2)\n",
    "bigrams = [(text2[i],text2[i+1]) for i in range(len(text2)-1)]\n",
    "bigram_counts = Counter(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ngrams(text,n):\n",
    "    if len(text)!=n:\n",
    "        return [tuple(text[i:(i+n)]) for i in range(len(text)-n+1)]\n",
    "    else:\n",
    "        return [tuple(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ngram_counts(text,n):\n",
    "    ngram_counts = Counter(compute_ngrams(text,n))\n",
    "    context_counts = Counter(compute_ngrams(text,n-1))\n",
    "    return ngram_counts,context_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('and', 'Sensibility', 'by')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams = compute_ngrams(text2,3)\n",
    "trigrams[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bigram_probability(bigram,bigram_counts,unigram_counts):\n",
    "    return bigram_counts[bigram]/unigram_counts[bigram[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ngram_probability(ngram,ngram_counts,context_counts,verbose=False):\n",
    "    if verbose:\n",
    "        print('ngram: ',ngram)\n",
    "        print('ngram counts: ',ngram_counts[ngram])\n",
    "        print('context: ',ngram[:len(ngram)-1])\n",
    "        print('context counts: ',context_counts[ngram[:len(ngram)-1]])\n",
    "    return ngram_counts[ngram]/context_counts[ngram[:len(ngram)-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_bigram_probability(s,bigram_counts,unigram_counts):\n",
    "    s = s.split(' ')\n",
    "    return np.exp(np.sum([np.log(compute_bigram_probability((s[i],s[i+1]),bigram_counts,unigram_counts)) for i in range(len(s)-1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_ngram_probability(s,ngram_counts,context_counts):\n",
    "    s = s.split(' ')\n",
    "    ngrams = compute_ngrams(s,len(list(ngram_counts.keys())[0]))\n",
    "    return np.exp(np.sum([np.log(compute_ngram_probability(ngrams[i],ngram_counts,context_counts)) for i in range(len(ngrams))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_counts,context_counts = compute_ngram_counts(text2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_counts[('she','did')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.569205742850947e-05"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_sentence_bigram_probability('she did not care',bigram_counts,unigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011299435028248582"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_sentence_ngram_probability('she did not care',ngram_counts,context_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(text,ngram_counts,context_counts):\n",
    "    ngrams = compute_ngrams(text,len(list(ngram_counts.keys())[0]))\n",
    "    return np.exp(-(1/len(text))*np.sum([np.log(compute_ngram_probability(ngrams[i],ngram_counts,context_counts)) for i in range(len(ngrams))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.6990318881662745"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_perplexity(text2,ngram_counts,context_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n= 2 : 37.37576749369875\n",
      "n= 3 : 4.6990318881662745\n",
      "n= 4 : 1.5042243590107542\n",
      "n= 5 : 1.095325619353706\n"
     ]
    }
   ],
   "source": [
    "for i in range(2,6):\n",
    "    ngram_counts,context_counts = compute_ngram_counts(text2,i)\n",
    "    print('n=',i,':',compute_perplexity(text2,ngram_counts,context_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_encode_text_train_test(min_count,train_text,test_text=[]):\n",
    "    train_unigram_counts = Counter(train_text)\n",
    "    train_rare_words = [w for w in list(train_unigram_counts.keys()) if train_unigram_counts[w]<=min_count]\n",
    "    train_text = ['<UNK>' if w in train_rare_words else w for w in train_text ]\n",
    "    test_text = ['<UNK>' if w not in list(train_unigram_counts.keys()) else w for w in test_text]\n",
    "    return train_text,test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text,test_text = unk_encode_text_train_test(min_count=1,train_text=text2,test_text=text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_counts,context_counts = compute_ngram_counts(train_text,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_bigram_smoothing_counts(bigram_counts,unigram_counts,k=1):\n",
    "    vocabulary = [l[0] for l in list(unigram_counts.keys())]\n",
    "    all_bigrams  = list(itertools.product(vocabulary,vocabulary))\n",
    "    all_bigram_counts = Counter(all_bigrams)\n",
    "    all_bigram_counts.update(Counter(dict.fromkeys(all_bigram_counts,k-1)))\n",
    "    all_bigram_counts.update(bigram_counts)\n",
    "    unigram_counts.update(Counter(dict.fromkeys(unigram_counts,len(vocabulary))))\n",
    "    return all_bigram_counts,unigram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace_bigram_counts,laplace_unigram_counts = laplace_bigram_smoothing_counts(ngram_counts,context_counts,k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word_from_context(context_word,ngram_counts):\n",
    "    ngrams = list(ngram_counts.keys())\n",
    "    context_ngrams = np.array([ngram for ngram in ngrams if ngram[0]==context_word])\n",
    "    next_possible_words = np.array([ngram[1] for ngram in context_ngrams])\n",
    "    context_probs = np.array([ngram_counts[ngram] for ngram in ngrams if ngram[0]==context_word],dtype='float')\n",
    "    context_probs/=np.sum(context_probs)\n",
    "    return np.random.choice(a=next_possible_words,size=1,p=context_probs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_sentence(seed_word,ngram_counts,stop_word='.',sentence_max_length=300,):\n",
    "    sentence = seed_word\n",
    "    next_word = sample_word_from_context(context_word=seed_word,ngram_counts=ngram_counts)\n",
    "    sentence += ' '+next_word\n",
    "    while next_word!='.' and len(sentence.split(' '))<300:\n",
    "        next_word = sample_word_from_context(context_word=seed_word,ngram_counts=ngram_counts)\n",
    "        sentence += ' '+next_word\n",
    "    print(\"Random Sentence:\",sentence)\n",
    "    return None        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Sentence: She was had would could blushed saw had will instantly smiled was perceived was began feared was <UNK> felt said began was instantly was was loved then managed had felt determined had was continued surprised began only will determined was expressly instantly saw was continued paused had had could felt saw does speedily sat had <UNK> was was had saw felt was did performed dared will liked was was received expressly even tried was fell expects was moved was might had doubted was was spent walked began sometimes wondered had trembled got met was acknowledged put had was concluded knows hesitated would came then thought returned had could insisted would observed would could saw declared could says had repeated blushed was dreaded was paused was will was acknowledged speedily was trembled knew was liked was was had must saw could is observed was caught paused instantly saw <UNK> looked tried was was began thanked turned had knew could looked instantly could instantly was could was speedily was would could used hesitated resigned said was had instantly was was had looked was went saw had must does was was speedily insisted now recommended could <UNK> had was expects has saw attended had thanked had gave is would paused was had was might would said dreaded has surprised felt looked , walked continued had was was could surprised seems was was was would would had thanked found does treated was was was is was thanked had <UNK> had left was was was was played sat could joined was would will returned got had had perceived had attended rather determined expected had saw perceived needed dared avoided seems came <UNK> played <UNK> paused was had was took said smiled managed walked speedily is acknowledged does speedily could was had left looked will was\n"
     ]
    }
   ],
   "source": [
    "generate_random_sentence(seed_word='She',ngram_counts=ngram_counts,stop_word='.',sentence_max_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
